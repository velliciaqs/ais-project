{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "486511f4-6ebb-4b73-a3c5-d380c6b24507",
   "metadata": {},
   "source": [
    "## Data AIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83508b71-db1b-4a47-a114-b790f353389e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For 3.3.1\n",
    "#Register Sedona Functions to Spark\n",
    "from sedona.register import SedonaRegistrator\n",
    "SedonaRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94f564e4-f04f-479d-a3d7-640b33fd3165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For 3.3.2\n",
    "from shapely.geometry import Point, Polygon, mapping\n",
    "import h3.api.numpy_int as h3int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e6a0182-706b-4eaa-8a14-8250098eb573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec83d56d-857b-4b6c-a32f-4b9eb6acf63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path\n",
    "base_path = \"s3a://ungp-ais-data-historical-backup/user_temp/\"\n",
    "path_unique = base_path + \"212112409/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe9e00a5-d8f6-4f23-8938-b17a4915529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "data_ais = spark.read.parquet(path_unique + \"ais-data-indonesia-2021.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7883b0b6-4da7-48cf-a267-1369a3e1cb4d",
   "metadata": {},
   "source": [
    "## Eksplorasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44ae4fd4-47d6-4077-b3ff-65459ebb513e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing down clientserver connection\n",
      "Collecting xlsxwriter\n",
      "  Downloading XlsxWriter-3.2.2-py3-none-any.whl (165 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xlsxwriter\n",
      "Successfully installed xlsxwriter-3.2.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "134e4af1-1efb-4958-9eb0-fdd8c84e819e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark/work-dir\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ddbb074-4fd7-4837-bcd9-a3b9f544ce39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Path (<tt>ais-data-indonesia-2021.parquet</tt>) doesn't exist. It may still be in the process of being generated, or you may have the incorrect path."
      ],
      "text/plain": [
       "/opt/spark/work-dir/ais-data-indonesia-2021.parquet"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "# Buat link download untuk file Parquet yang telah disimpan\n",
    "FileLink('ais-data-indonesia-2021.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d99353-41df-45b8-9872-8e6213436046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while sending or receiving.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 503, in send_command\n",
      "    self.socket.sendall(command.encode(\"utf-8\"))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Closing down clientserver connection\n",
      "Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 503, in send_command\n",
      "    self.socket.sendall(command.encode(\"utf-8\"))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 506, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending\n",
      "Closing down clientserver connection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from IPython.display import HTML\n",
    "\n",
    "def create_download_link(df, sheet_name, title, filename):\n",
    "    # Konversi DataFrame PySpark ke Pandas\n",
    "    df_pandas = df.toPandas()\n",
    "    \n",
    "    # Simpan ke file Excel dalam memori\n",
    "    output = BytesIO()\n",
    "    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:\n",
    "        df_pandas.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    \n",
    "    # Encode file Excel ke base64\n",
    "    output.seek(0)\n",
    "    b64 = base64.b64encode(output.read()).decode()\n",
    "    \n",
    "    # Buat link download\n",
    "    html = f'<a download=\"{filename}\" href=\"data:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;base64,{b64}\" target=\"_blank\">{title}</a>'\n",
    "    \n",
    "    return HTML(html)\n",
    "\n",
    "# Contoh penggunaan untuk satu DataFrame\n",
    "create_download_link(data_ais, \"2021\", \"Download Data\", \"data-ais-2021.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af7bf8d-0f98-49e7-b439-f5fd9407ba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark3.3 ais2.8 (prev ais-tt-dev)",
   "language": "python3",
   "name": "ais-tt-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
